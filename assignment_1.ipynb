{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GizawAAiT/NLP/blob/main/assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-0fymdVZ2ux"
      },
      "source": [
        "## *Read The Corpus (GPAC)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MIK465qtedu",
        "outputId": "356e3e0d-a7e7-415d-f62a-54e2f36673d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{33: None, 34: None, 35: None, 36: None, 37: None, 38: None, 39: None, 40: None, 41: None, 42: None, 43: None, 44: None, 45: None, 46: None, 47: None, 58: None, 59: None, 60: None, 61: None, 62: None, 63: None, 64: None, 91: None, 92: None, 93: None, 94: None, 95: None, 96: None, 123: None, 124: None, 125: None, 126: None, 4962: None, 4964: None, 4965: None, 4966: None, 4967: None, 4968: None, 4961: None, 4963: None}\n",
            "    ምን መሰላችሁ? (አንባቢያን) ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው ያልቻለችው የአለም የእግር ኳስ ዋ ለ19ኛ ጊዜ በደቡብ አፍሪካ ሲጠጣ፣ በሩቅ እያየች አንጀቷ ባረረ ልክ በአመቱ በለስ ቀናትና ሌላ ዋ ልትታደም ሁለት ልጆቿን ወደ ደቡብ አፍሪካ ላከች፡፡6ኛው ቢግ ብራዘርስ አፍሪካ አብሮ የመኖር ውድድር \n",
            "     ምን መሰላችሁ አንባቢያን ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው ያልቻለችው የአለም የእግር ኳስ ዋ ለ19ኛ ጊዜ በደቡብ አፍሪካ ሲጠጣ በሩቅ እያየች አንጀቷ ባረረ ልክ በአመቱ በለስ ቀናትና ሌላ ዋ ልትታደም ሁለት ልጆቿን ወደ ደቡብ አፍሪካ ላከች6ኛው ቢግ ብራዘርስ አፍሪካ አብሮ የመኖር ውድድር በደቡብ \n",
            "96481\n"
          ]
        }
      ],
      "source": [
        "# import\n",
        "from google.colab import drive\n",
        "import textwrap, string, re\n",
        "\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Specify the corpus path in the drive:\n",
        "corpus_path = '/content/drive/MyDrive/NLP/GPAC.txt'\n",
        "\n",
        "# Open the file and read its content\n",
        "with open(corpus_path, 'r', encoding='utf-8') as file:\n",
        "        # Read the entire content of the file\n",
        "        corpus_content = file.read(100000000)\n",
        "\n",
        "        # Amharic punctuation characters to be removed\n",
        "        amharic_punctuation = \"።፤፥፦፧፨።!;:?,፡'\\\"፣\"\n",
        "\n",
        "        # Create a translation table.\n",
        "        translator = str.maketrans('', '', string.punctuation + amharic_punctuation)\n",
        "        print(translator)\n",
        "\n",
        "        # Remove punctuation using the translation table\n",
        "        content_without_punctuation = corpus_content.translate(translator)\n",
        "        print(corpus_content[:200], \"\\n\",content_without_punctuation[:200])\n",
        "\n",
        "print(len(content_without_punctuation))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94k3yjXvZC93"
      },
      "source": [
        "# 1.1 Create n-grams for n=1, 2, 3, 4. You can show sample prints.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "JdLYl4Kcabsb"
      },
      "outputs": [],
      "source": [
        "class Corpus:\n",
        "  def __init__(self, content):\n",
        "    self.content = content\n",
        "    self.words = self.content.split()\n",
        "    self.uniqueWords = set(self.words)\n",
        "\n",
        "  def ngram(self, n):\n",
        "    ngrams_list = []\n",
        "\n",
        "    for num in range(0, len(self.words)-n):\n",
        "        ngram = tuple(self.words[num:num + n])\n",
        "        ngrams_list.append(ngram)\n",
        "\n",
        "    return ngrams_list\n",
        "\n",
        "  # n-gram setters for n (1, 2, 3, 4)\n",
        "  def set_unigrams(self):\n",
        "    self.oneGrams = self.ngram(1)\n",
        "\n",
        "  def set_bigrams(self):\n",
        "    self.twoGrams = self.ngram(2)\n",
        "\n",
        "  def set_trigrams(self):\n",
        "    self.threeGrams = self.ngram(3)\n",
        "\n",
        "  def set_quadgrams(self):\n",
        "    self.fourGrams = self.ngram(4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8oBmXS2boe1",
        "outputId": "63011068-c66f-4d89-9795-6ddeafd39471"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('ምን',), ('መሰላችሁ',), ('አንባቢያን',), ('ኢትዮጵያ',), ('በተደጋጋሚ',), ('ጥሪው',), ('ደርሷት',), ('ልትታደመው',), ('ያልቻለችው',), ('የአለም',)]\n",
            "[('ምን', 'መሰላችሁ'), ('መሰላችሁ', 'አንባቢያን'), ('አንባቢያን', 'ኢትዮጵያ'), ('ኢትዮጵያ', 'በተደጋጋሚ'), ('በተደጋጋሚ', 'ጥሪው'), ('ጥሪው', 'ደርሷት'), ('ደርሷት', 'ልትታደመው'), ('ልትታደመው', 'ያልቻለችው'), ('ያልቻለችው', 'የአለም'), ('የአለም', 'የእግር')]\n",
            "[('ምን', 'መሰላችሁ', 'አንባቢያን'), ('መሰላችሁ', 'አንባቢያን', 'ኢትዮጵያ'), ('አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ'), ('ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው'), ('በተደጋጋሚ', 'ጥሪው', 'ደርሷት'), ('ጥሪው', 'ደርሷት', 'ልትታደመው'), ('ደርሷት', 'ልትታደመው', 'ያልቻለችው'), ('ልትታደመው', 'ያልቻለችው', 'የአለም'), ('ያልቻለችው', 'የአለም', 'የእግር'), ('የአለም', 'የእግር', 'ኳስ')]\n",
            "[('ምን', 'መሰላችሁ', 'አንባቢያን', 'ኢትዮጵያ'), ('መሰላችሁ', 'አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ'), ('አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው'), ('ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው', 'ደርሷት'), ('በተደጋጋሚ', 'ጥሪው', 'ደርሷት', 'ልትታደመው'), ('ጥሪው', 'ደርሷት', 'ልትታደመው', 'ያልቻለችው'), ('ደርሷት', 'ልትታደመው', 'ያልቻለችው', 'የአለም'), ('ልትታደመው', 'ያልቻለችው', 'የአለም', 'የእግር'), ('ያልቻለችው', 'የአለም', 'የእግር', 'ኳስ'), ('የአለም', 'የእግር', 'ኳስ', 'ዋ')]\n"
          ]
        }
      ],
      "source": [
        "# create corpus object\n",
        "corpus = Corpus(content_without_punctuation)\n",
        "\n",
        "# Set the n grams (1, 2, 3, 4) for the corpus obj.\n",
        "corpus.set_unigrams()\n",
        "corpus.set_bigrams()\n",
        "corpus.set_trigrams()\n",
        "corpus.set_quadgrams()\n",
        "\n",
        "# sample print\n",
        "print(corpus.oneGrams[:10])\n",
        "print(corpus.twoGrams[:10])\n",
        "print(corpus.threeGrams[:10])\n",
        "print(corpus.fourGrams[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gMKUOAyUD7I"
      },
      "source": [
        "#1.2 Calculate probabilities of n-grams and find the top 10 most likely n-grams for all n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmLLD2WvV7VD",
        "outputId": "eb6a796c-c5bb-4cc2-d24d-2bc49620401a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('ነው',), 0.015273838097313041), (('ላይ',), 0.010473488981014658), (('ወደ',), 0.00627318350425357), (('እንደ',), 0.0061095352389252166), (('ጋር',), 0.005836788130044626), (('ነበር',), 0.005564041021164037), (('ም',), 0.004200305476761087), (('ዓ',), 0.004145756054984968), (('ጊዜ',), 0.00409120663320885), (('እና',), 0.00409120663320885)]\n",
            "[(('ዓ', 'ም'), 0.004145982215917477), (('ጋር', 'ሆኖ'), 0.0013638099394465387), (('በስድስት', 'ወር'), 0.0013638099394465387), (('ብቻ', 'ሳይሆን'), 0.001091047951557231), (('ብላታ', 'ወልደጊዮርጊስ'), 0.001091047951557231), (('ሀሁ', 'በስድስት'), 0.001091047951557231), (('ወይም', 'ፐፑ'), 0.0008182859636679231), (('ከበደ', 'ሚካኤል'), 0.0008182859636679231), (('ሀሁ', 'ወይም'), 0.0008182859636679231), (('ወደ', 'ኋላ'), 0.0007091811685122001)]\n",
            "[(('ሀሁ', 'በስድስት', 'ወር'), 0.0010911074740858997), (('ሀሁ', 'ወይም', 'ፐፑ'), 0.0008183306055644248), (('ጋር', 'ሆኖ', 'ኢህአዴግን'), 0.0005455537370429498), (('ያም', 'ሆነ', 'ይህ'), 0.0005455537370429498), (('ዓ', 'ም', 'ከደርግ'), 0.0005455537370429498), (('ዓ', 'ም', 'ነው'), 0.0005455537370429498), (('ከኢህአዴግ', 'ጋር', 'ሆኖ'), 0.0005455537370429498), (('እናት', 'ዓለም', 'ጠኑ'), 0.0005455537370429498), (('በብርሃንና', 'ሰላም', 'ማተሚያ'), 0.0005455537370429498), (('በ1967', 'ዓ', 'ም'), 0.0005455537370429498)]\n",
            "[(('ጋር', 'ሆኖ', 'ኢህአዴግን', 'ወጋ'), 0.0005455835015550718), (('በብርሃንና', 'ሰላም', 'ማተሚያ', 'ቤት'), 0.0005455835015550718), (('ከፖለቲካ', 'ያገኛቸው', 'ክብር', 'ተሰሚነት'), 0.0004364668012440575), (('ፖለቲካው', 'የተቀላቀሉት', 'ገና', 'ልጅ'), 0.0002727917507775359), (('ፖለቲካው', 'ዓለም', 'የገባው', 'ጋዜጠኝነትን'), 0.0002727917507775359), (('ፖለቲካዊ', 'አቋሙ', 'ነው', 'ለአስር'), 0.0002727917507775359), (('ፕሬስ', 'ድርጅት', 'ላይ', 'ሲነግስ'), 0.0002727917507775359), (('ፐፑ', 'እንደመጀመሪያው', 'አብዮት', 'ማጠናከሪያ'), 0.0002727917507775359), (('ፐፑ', 'ን', 'ለማሳየት', 'ቢሞከርም'), 0.0002727917507775359), (('ፐፑ', 'ሀሁ', 'ወይም', 'ፐፑ'), 0.0002727917507775359)]\n"
          ]
        }
      ],
      "source": [
        "from heapq import heapify, heappush, heappop\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "def probability(grams):\n",
        "    frequency = Counter(grams)\n",
        "    total = len(grams)\n",
        "    probability = {gram: freq/total for gram, freq in frequency.items()}\n",
        "    return  probability\n",
        "\n",
        "\"\"\"\n",
        "Use heap Data structure to maintain the 10 most frequent n-grams with just one iteration.\n",
        "\"\"\"\n",
        "def top_10_probability(probability):\n",
        "    h = [] #heap:\n",
        "\n",
        "    for gram in probability:\n",
        "      heappush(h, (probability[gram], gram))\n",
        "      if len(h) > 10:\n",
        "        heappop(h)\n",
        "\n",
        "    top_10, total = [], sum(probability.values())\n",
        "    while h:\n",
        "      gram = heappop(h)[1]\n",
        "      top_10.append((gram, probability[gram]/total))\n",
        "\n",
        "\n",
        "    return list(reversed(top_10))\n",
        "\n",
        "# calculate the probability for each n-grams [1, 2, 3, 4]\n",
        "oneGramProb = probability(corpus.oneGrams)\n",
        "twoGramProb = probability(corpus.twoGrams)\n",
        "threeGramProb = probability(corpus.threeGrams)\n",
        "fourGramProb = probability(corpus.fourGrams)\n",
        "\n",
        "# print top 10 probability for each\n",
        "print(top_10_probability(oneGramProb))\n",
        "print(top_10_probability(twoGramProb))\n",
        "print(top_10_probability(threeGramProb))\n",
        "print(top_10_probability(fourGramProb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFtfHivYop6I"
      },
      "source": [
        "# 1.3 What is the probability of the sentence. \"ኢትዮጵያ ታሪካዊ ሀገር ናት \". You can also try\n",
        "more sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5HZ_kpqW4aL",
        "outputId": "24509017-f806-49af-9b65-95fd5e6418e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.225\n",
            "0.225\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "def sentenceProbabilityWithBigram(sentence, corpus):\n",
        "    oneGramFreq, twoGramFreq = Counter(corpus.oneGrams), Counter(corpus.twoGrams)\n",
        "    words = sentence.split()\n",
        "    probability = 1.0\n",
        "\n",
        "    for i in range(len(words)-1):\n",
        "        bigram_coutnt, unigram_count = twoGramFreq[(words[i], words[i+1])], oneGramFreq[(words[i],)]\n",
        "        probability *= bigram_coutnt/unigram_count if unigram_count !=0 else 1e-10 # 1e-10 is for smoothing for unseen n-grams or n-grams with zero counts.\n",
        "    return probability\n",
        "\n",
        "prob = sentenceProbabilityWithBigram(\"ነገር ግን\", corpus)\n",
        "print(prob)\n",
        "\n",
        "def sentenceProbabilityWith_ngrams(sentence, corpus, n):\n",
        "    ngramsFreq = Counter(corpus.ngram(n))\n",
        "    nMinusOneGramFreq = Counter(corpus.ngram(n-1))\n",
        "    words = sentence.split()\n",
        "    prob = 1.0\n",
        "    for i in range(len(words)-n+1):\n",
        "        ngram_count, nMinuseOneGramCount = ngramsFreq[tuple(words[i:i+n])], nMinusOneGramFreq[tuple(words[i:i+n-1])]\n",
        "        prob *= ngram_count / nMinuseOneGramCount if nMinuseOneGramCount != 0 else 1e-10\n",
        "\n",
        "    return prob\n",
        "print(sentenceProbabilityWith_ngrams(\"ነገር ግን\", corpus, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.4 Generate random sentences using n-grams; explain what happens as n-increases\n",
        "based on your output."
      ],
      "metadata": {
        "id": "3YsrJeppV7Zl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "wGsX0DmxYG-1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17e77737-4bb5-4bd6-a31c-feeb264a0f23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "በሳሚ ዓይን ተነበበ በዓይኖቼ ዓይኗን ስመረምር የሳሚን ዓይን ተነበበ በዓይኖቼ ዓይኗን\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "\"\"\"\n",
        "Generate a random text using n gram probability.\n",
        "At start, if the sentence length is less than n, i will just use bigram.\n",
        "\"\"\"\n",
        "def generateRandomSentenceWithNGramProb(corpus, length, n):\n",
        "    # randomly generate the start of all cnt sentences:\n",
        "    sentences = [random.choice(corpus.words)]\n",
        "\n",
        "    # define bigram and ngram frequency:\n",
        "    bigram_freq, ngram_freq = Counter(corpus.ngram(2)), Counter(corpus.ngram(n))\n",
        "    # print(corpus.uniqueWords)\n",
        "    for _ in range(length-n):\n",
        "        ngram_candidate = [word for word in corpus.uniqueWords if tuple(sentences[-n+1:]) + tuple([word]) in ngram_freq.keys()]\n",
        "\n",
        "        bigram_candidate = [word for word in corpus.uniqueWords if tuple(sentences[-1:]) + tuple([word]) in bigram_freq.keys()]\n",
        "\n",
        "        # try by n gram\n",
        "        if ngram_candidate:\n",
        "            sentences.append(random.choice(ngram_candidate))\n",
        "\n",
        "        # if not, what about using bigram\n",
        "        elif bigram_candidate:\n",
        "            sentences.append(random.choice(bigram_candidate))\n",
        "\n",
        "        # otherwise use bigram\n",
        "        else:\n",
        "            break  # If there are no candidates, stop generating\n",
        "\n",
        "    return ' '.join(sentences)\n",
        "random_sent = generateRandomSentenceWithNGramProb(corpus, 40, 30)\n",
        "print(random_sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis:\n",
        "\n",
        "# Higher n Values (e.g., n=30):\n",
        "## Pros:\n",
        "\n",
        "*Increased context dependence leads to more meaningful and coherent sentences.\n",
        "Enhanced coherence and comprehensibility due to richer context.*\n",
        "## Cons:\n",
        "\n",
        "Difficulty in completing sentences of specified length.\n",
        "Higher sensitivity to specific sequences may result in data sparsity issues.\n",
        "# Lower n Values (e.g., n=3, 4):\n",
        "## Pros:\n",
        "\n",
        "Guaranteed full-length sentences with more flexibility in word choice.\n",
        "## Cons:\n",
        "\n",
        "Limited context dependence leads to decreased coherence.\n",
        "Sacrifice in overall semantic richness.\n",
        "Trade-offs and Considerations:\n",
        "# Balancing Act:\n",
        "Trade-off between coherence and the ability to generate complete sentences.\n",
        "Optimal n depends on task requirements and characteristics of the training data."
      ],
      "metadata": {
        "id": "FXn1D_cjqC8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Evaluate these Language Models Using Intrinsic Evaluation Method (4%)"
      ],
      "metadata": {
        "id": "1crM0_HJsrhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from math import log\n",
        "\n",
        "def calculate_perplexity(test_set, ngram_probabilities):\n",
        "    total_log_prob = 0\n",
        "    N = len(test_set)\n",
        "\n",
        "    for ngram in test_set:\n",
        "        prob = ngram_probabilities.get(ngram, 1e-10)  # Smoothing for unseen n-grams\n",
        "        total_log_prob += -1 * (1/N) * (prob and log(prob)) or 0\n",
        "\n",
        "    perplexity = 2 ** total_log_prob\n",
        "    return perplexity\n",
        "\n",
        "# Sample test set (2, 3, 4, ... grams)\n",
        "n = 2\n",
        "test_set = corpus.ngram(n)\n",
        "\n",
        "# Calculate perplexity for bigram model\n",
        "ngram_probability = {gram: random.uniform(0, 1) for gram in corpus.ngram(n)}\n",
        "perplexity = calculate_perplexity(test_set, ngram_probability)\n",
        "\n",
        "print(f\"{n} gram Perplexity: {perplexity}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "QcwE6Ewksgte",
        "outputId": "7d5bbd37-9585-4f34-d42d-5ed94f6730e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 gram Perplexity: 2.011897882762724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Evaluate these Language Models Using Extrinsic Evaluation Method (4%)"
      ],
      "metadata": {
        "id": "X6Tf5NDzx1RG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g11Gb9phx27O"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}